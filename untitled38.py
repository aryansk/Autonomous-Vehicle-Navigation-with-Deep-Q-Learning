# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eYRK9MFlBHCVOoOvlIyHT8CAF2kB_ebo
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
from gymnasium import spaces
import pygame
import random
from collections import deque, namedtuple
import matplotlib.pyplot as plt
import time
import seaborn as sns
from matplotlib.patches import Rectangle, Circle

# Constants
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 600
VEHICLE_SIZE = 20
OBSTACLE_SIZE = 30
N_OBSTACLES = 10

# Colors
BLUE = (0, 0, 255)
RED = (255, 0, 0)
GREEN = (0, 255, 0)
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)

class Visualizer:
    def __init__(self):
        pygame.init()
        self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
        pygame.display.set_caption("Autonomous Vehicle Navigation")
        self.clock = pygame.time.Clock()
        self.font = pygame.font.Font(None, 36)

    def render(self, vehicle_pos, obstacles, goal, reward, episode, step, epsilon):
        self.screen.fill(WHITE)

        # Draw obstacles
        for obs in obstacles:
            pygame.draw.rect(self.screen, RED,
                           (obs[0], obs[1], OBSTACLE_SIZE, OBSTACLE_SIZE))

        # Draw goal
        pygame.draw.circle(self.screen, GREEN,
                         (int(goal[0]), int(goal[1])), VEHICLE_SIZE//2)

        # Draw vehicle
        pygame.draw.rect(self.screen, BLUE,
                        (vehicle_pos[0], vehicle_pos[1], VEHICLE_SIZE, VEHICLE_SIZE))

        # Draw info text
        info_text = [
            f"Episode: {episode}",
            f"Step: {step}",
            f"Reward: {reward:.1f}",
            f"Epsilon: {epsilon:.2f}"
        ]

        for i, text in enumerate(info_text):
            text_surface = self.font.render(text, True, BLACK)
            self.screen.blit(text_surface, (10, 10 + i * 30))

        pygame.display.flip()
        self.clock.tick(30)

    def close(self):
        pygame.quit()

class VehicleEnv(gym.Env):
    def __init__(self, render_mode=None):
        super().__init__()

        # Action space: [left, right, up, down]
        self.action_space = spaces.Discrete(4)

        # Observation space: vehicle position (x,y) and obstacles positions (x,y)
        self.observation_space = spaces.Box(
            low=0,
            high=max(SCREEN_WIDTH, SCREEN_HEIGHT),
            shape=(2 + 2 * N_OBSTACLES,),
            dtype=np.float32
        )

        self.render_mode = render_mode
        if render_mode == 'human':
            self.visualizer = Visualizer()

        self.reset()

    def reset(self, seed=None):
        super().reset(seed=seed)
        # Initialize vehicle position
        self.vehicle_pos = np.array([SCREEN_WIDTH//2, SCREEN_HEIGHT-50], dtype=np.float32)

        # Initialize obstacles
        self.obstacles = []
        for _ in range(N_OBSTACLES):
            x = random.randint(0, SCREEN_WIDTH-OBSTACLE_SIZE)
            y = random.randint(0, SCREEN_HEIGHT-200)  # Keep some space at bottom
            self.obstacles.append([x, y])

        self.goal = np.array([SCREEN_WIDTH//2, 50])  # Goal at top center
        self.steps = 0
        self.current_reward = 0
        return self._get_obs(), {}

    def _get_obs(self):
        obs = np.concatenate([self.vehicle_pos] + [np.array(obs) for obs in self.obstacles])
        return obs.astype(np.float32)

    def step(self, action):
        self.steps += 1
        movement = {
            0: [-5, 0],   # Left
            1: [5, 0],    # Right
            2: [0, -5],   # Up
            3: [0, 5]     # Down
        }

        # Move vehicle
        self.vehicle_pos += movement[action]

        # Clip position to screen bounds
        self.vehicle_pos = np.clip(
            self.vehicle_pos,
            [0, 0],
            [SCREEN_WIDTH-VEHICLE_SIZE, SCREEN_HEIGHT-VEHICLE_SIZE]
        )

        # Check collisions
        terminated = False
        reward = -0.1  # Small negative reward for each step

        # Check goal
        if np.linalg.norm(self.vehicle_pos - self.goal) < VEHICLE_SIZE:
            reward = 100
            terminated = True

        # Check obstacles
        vehicle_rect = pygame.Rect(
            self.vehicle_pos[0], self.vehicle_pos[1],
            VEHICLE_SIZE, VEHICLE_SIZE
        )

        for obs in self.obstacles:
            obs_rect = pygame.Rect(obs[0], obs[1], OBSTACLE_SIZE, OBSTACLE_SIZE)
            if vehicle_rect.colliderect(obs_rect):
                reward = -50
                terminated = True

        # Max steps termination
        if self.steps >= 1000:
            terminated = True

        self.current_reward = reward

        if self.render_mode == 'human':
            self.render()

        return self._get_obs(), reward, terminated, False, {}

    def render(self):
        if self.render_mode == 'human':
            self.visualizer.render(
                self.vehicle_pos, self.obstacles, self.goal,
                self.current_reward, 0, self.steps, 0
            )

    def close(self):
        if self.render_mode == 'human':
            self.visualizer.close()

class DQN(nn.Module):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_size)
        )

    def forward(self, x):
        return self.network(x)

class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = deque(maxlen=capacity)
        self.Experience = namedtuple('Experience',
            field_names=['state', 'action', 'reward', 'next_state', 'done'])

    def add(self, state, action, reward, next_state, done):
        experience = self.Experience(state, action, reward, next_state, done)
        self.memory.append(experience)

    def sample(self, batch_size):
        experiences = random.sample(self.memory, k=batch_size)

        states = torch.from_numpy(
            np.vstack([e.state for e in experiences if e is not None])).float()
        actions = torch.from_numpy(
            np.vstack([e.action for e in experiences if e is not None])).long()
        rewards = torch.from_numpy(
            np.vstack([e.reward for e in experiences if e is not None])).float()
        next_states = torch.from_numpy(
            np.vstack([e.next_state for e in experiences if e is not None])).float()
        dones = torch.from_numpy(
            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.memory)

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

        # Q-Networks
        self.qnetwork_local = DQN(state_size, action_size)
        self.qnetwork_target = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.qnetwork_local.parameters())

        # Replay memory
        self.memory = ReplayBuffer(10000)
        self.batch_size = 64

        # Training parameters
        self.gamma = 0.99
        self.tau = 1e-3
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995

    def act(self, state, eval_mode=False):
        state = torch.from_numpy(state).float().unsqueeze(0)

        self.qnetwork_local.eval()
        with torch.no_grad():
            action_values = self.qnetwork_local(state)
        self.qnetwork_local.train()

        # Epsilon-greedy action selection
        if random.random() > self.epsilon or eval_mode:
            return np.argmax(action_values.cpu().data.numpy())
        return random.choice(np.arange(self.action_size))

    def step(self, state, action, reward, next_state, done):
        self.memory.add(state, action, reward, next_state, done)

        if len(self.memory) > self.batch_size:
            experiences = self.memory.sample(self.batch_size)
            self.learn(experiences)

    def learn(self, experiences):
        states, actions, rewards, next_states, dones = experiences

        # Get max predicted Q values for next states from target model
        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))

        # Get current Q values from local model
        Q_expected = self.qnetwork_local(states).gather(1, actions)

        # Compute loss
        loss = nn.MSELoss()(Q_expected, Q_targets)

        # Minimize the loss
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Update target network
        self.soft_update(self.qnetwork_local, self.qnetwork_target)

        # Update epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def soft_update(self, local_model, target_model):
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(
                self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

def plot_training_metrics(scores, epsilons, success_rate):
    plt.figure(figsize=(15, 10))

    # Plot scores
    plt.subplot(311)
    plt.plot(scores, label='Score', color='blue', alpha=0.6)
    plt.plot(np.convolve(scores, np.ones(100)/100, mode='valid'),
             label='Average Score (100 ep)', color='red')
    plt.title('Training Progress')
    plt.xlabel('Episode')
    plt.ylabel('Score')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot epsilon decay
    plt.subplot(312)
    plt.plot(epsilons, label='Epsilon', color='green')
    plt.title('Exploration Rate Decay')
    plt.xlabel('Episode')
    plt.ylabel('Epsilon')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot success rate
    plt.subplot(313)
    plt.plot(success_rate, label='Success Rate', color='purple')
    plt.title('Success Rate (100 episode window)')
    plt.xlabel('Episode')
    plt.ylabel('Success Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_metrics.png')
    plt.close()

def plot_path_heatmap(env, agent, n_episodes=100):
    positions = []
    plt.figure(figsize=(10, 8))

    for _ in range(n_episodes):
        state, _ = env.reset()
        positions.append(env.vehicle_pos.copy())

        while True:
            action = agent.act(state, eval_mode=True)
            state, _, terminated, truncated, _ = env.step(action)
            positions.append(env.vehicle_pos.copy())

            if terminated or truncated:
                break

    positions = np.array(positions)

    # Create heatmap
    plt.hist2d(positions[:, 0], positions[:, 1], bins=50, cmap='viridis')
    plt.colorbar(label='Visitation frequency')

    # Plot obstacles from last episode
    for obs in env.obstacles:
        plt.gca().add_patch(Rectangle((obs[0], obs[1]),
                                    OBSTACLE_SIZE, OBSTACLE_SIZE,
                                    color='red', alpha=0.5))

    # Plot goal
    plt.gca().add_patch(Circle((env.goal[0], env.goal[1]),
                              VEHICLE_SIZE//2, color='green', alpha=0.5))

    plt.title('Path Heatmap (100 Episodes)')
    plt.xlabel('X Position')
    plt.ylabel('Y Position')
    plt.savefig('path_heatmap.png')
    plt.close()

def train_agent():
    env = VehicleEnv(render_mode='human')  # Enable visualization
    agent = DQNAgent(
        state_size=env.observation_space.shape[0],
        action_size=env.action_space.n
    )

    n_episodes = 1000
    scores = []
    epsilons = []
    success_count = 0
    success_rate = []

    try:
        for i_episode in range(n_episodes):
            state, _ = env.reset()
            score = 0

            while True:
                action = agent.act(state)
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated

                agent.step(state, action, reward, next_state, done)
                state = next_state
                score += reward

                # Update visualization
                if env.render_mode == 'human':
                    env.visualizer.render(
                        env.vehicle_pos, env.obstacles, env.goal,
                        reward, i_episode, env.steps, agent.epsilon
                    )

                if done:
                    break

            # Track metrics
            scores.append(score)
            epsilons.append(agent.epsilon)
            success_count += 1 if score >= 100 else 0  # Count successful episodes

            if i_episode >= 99:  # Calculate success rate over last 100 episodes
                success_rate.append(success_count / 100)
                success_count -= 1 if scores[-100] >= 100 else 0
            else:
                success_rate.append(0)

            mean_score = np.mean(scores[-100:])

            if i_episode % 10 == 0:  # More frequent updates
                print(f'Episode {i_episode}\tAverage Score: {mean_score:.2f}\tEpsilon: {agent.epsilon:.2f}')
                # Plot metrics every 10 episodes
                plot_training_metrics(scores, epsilons, success_rate)

            if mean_score >= 195.0:
                print(f'\nEnvironment solved in {i_episode} episodes!')
                torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')
                break

        # Generate final visualizations
        plot_training_metrics(scores, epsilons, success_rate)
        plot_path_heatmap(env, agent)

    except KeyboardInterrupt:
        print("\nTraining interrupted by user")

    finally:
        env.close()
        return scores, epsilons, success_rate

if __name__ == "__main__":
    scores, epsilons, success_rate = train_agent()